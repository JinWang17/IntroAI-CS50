{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cloudy-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"C:/Users/wangj337/Google Drive/Courses/IntroAI-CS50/2Uncertainty/pagerank/pagerank.py\"\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy\n",
    "\n",
    "DAMPING = 0.85\n",
    "SAMPLES = 10000\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 2:\n",
    "        sys.exit(\"Usage: python pagerank.py corpus\")\n",
    "    corpus = crawl(sys.argv[1])\n",
    "    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)\n",
    "    print(f\"PageRank Results from Sampling (n = {SAMPLES})\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
    "    ranks = iterate_pagerank(corpus, DAMPING)\n",
    "    print(f\"PageRank Results from Iteration\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
    "\n",
    "\n",
    "def crawl(directory):\n",
    "    \"\"\"\n",
    "    Parse a directory of HTML pages and check for links to other pages.\n",
    "    Return a dictionary where each key is a page, and values are\n",
    "    a list of all other pages in the corpus that are linked to by the page.\n",
    "    \"\"\"\n",
    "    # The keys in that dictionary represent pages (e.g., \"2.html\"), and the values of the dictionary are a set of all of the pages linked to by the key (e.g. {\"1.html\", \"3.html\"}).\n",
    "    pages = dict()\n",
    "\n",
    "    # Extract all links from HTML files\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".html\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents = f.read()\n",
    "            links = re.findall(r\"<a\\s+(?:[^>]*?)href=\\\"([^\\\"]*)\\\"\", contents)\n",
    "            pages[filename] = set(links) - {filename}\n",
    "\n",
    "    # Only include links to other pages in the corpus\n",
    "    for filename in pages:\n",
    "        pages[filename] = set(\n",
    "            link for link in pages[filename]\n",
    "            if link in pages\n",
    "            # If there is a link to wiki in page 1, but wiki is not in the corpse folder\n",
    "            # then wiki will not be part of page 1's links. \n",
    "            # \"We only rank what we know for sure.\", said Jin.\n",
    "        )\n",
    "\n",
    "    return pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "adapted-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_model(corpus, page, damping_factor):\n",
    "    \"\"\"\n",
    "    Return a probability distribution over which page to visit next,\n",
    "    given a current page.\n",
    "\n",
    "    With probability `damping_factor`, choose a link at random\n",
    "    linked to by `page`. With probability `1 - damping_factor`, choose\n",
    "    a link at random chosen from all pages in the corpus.\n",
    "    \"\"\"\n",
    "    #print(corpus)\n",
    "    result = dict()\n",
    "    N = len(corpus)\n",
    "    N_page = len(corpus[page])\n",
    "    # ignore the link to its own page\n",
    "    if page in corpus[page]:\n",
    "        N_page -= 1\n",
    "\n",
    "    # With prob = 1 - damping_factor, randomly choose from all pages\n",
    "    for p in corpus:\n",
    "        result[p] = 1 / N * (1 - damping_factor)\n",
    "        \n",
    "    # With prob = damping_factor, randomly choose from links\n",
    "    for p in corpus[page]:\n",
    "        if p == page:\n",
    "            continue\n",
    "        else:\n",
    "            result[p] += 1 / N_page * damping_factor\n",
    "        \n",
    "    # Normalize for floating errors:\n",
    "    factor = 1.0 / sum(result.values())\n",
    "    for p in result:\n",
    "        result[p] = result[p]*factor\n",
    "\n",
    "    # The return value of the function should be a Python dictionary with one \n",
    "    # key for each page in the corpus. Each key should be mapped to a value \n",
    "    # representing the probability that a random surfer would choose that page next. \n",
    "    # The values in this returned probability distribution should sum to 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "knowing-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pagerank(corpus, damping_factor, n):\n",
    "    \"\"\"\n",
    "    Return PageRank values for each page by sampling `n` pages\n",
    "    according to transition model, starting with a page at random.\n",
    "\n",
    "    Return a dictionary where keys are page names, and values are\n",
    "    their estimated PageRank value (a value between 0 and 1). All\n",
    "    PageRank values should sum to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = list()\n",
    "    \n",
    "    # First sample: choosing from a page at random.\n",
    "    current = random.choice(list(corpus.keys()))\n",
    "    \n",
    "    # Next sample: generated from the previous sample based on the \n",
    "    # previous sampleâ€™s transition model.\n",
    "    for i in range(n):\n",
    "        trans = transition_model(corpus, current, damping_factor)\n",
    "        current = random.choices(\n",
    "            population = list(trans.keys()),\n",
    "            weights = trans.values(),\n",
    "            k = 1\n",
    "        )[0]\n",
    "        samples.append(current)     \n",
    "        \n",
    "    # Summary of samples    \n",
    "    frequency = Counter(samples)\n",
    "    for page in frequency:\n",
    "        frequency[page] = frequency[page] / n      \n",
    "    return frequency \n",
    "\n",
    "\n",
    "def iterate_pagerank(corpus, damping_factor):\n",
    "    \"\"\"\n",
    "    Return PageRank values for each page by iteratively updating\n",
    "    PageRank values until convergence.\n",
    "\n",
    "    Return a dictionary where keys are page names, and values are\n",
    "    their estimated PageRank value (a value between 0 and 1). All\n",
    "    PageRank values should sum to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocessing\n",
    "    N = len(corpus)\n",
    "    numLinks = numpy.array([len(corpus[p]) for p in corpus], dtype=float)\n",
    "    numLinks[numLinks == 0] = N\n",
    "    prob = numpy.reciprocal(numLinks)\n",
    "    binaryLinks = numpy.zeros((N, N))\n",
    "    for i, p1 in enumerate(corpus.keys()):\n",
    "        if not corpus[p1]:\n",
    "        # A page that has no links at all should be interpreted \n",
    "        # as having one link for every page in the corpus (including itself).\n",
    "            for j in range(N):\n",
    "                binaryLinks[j, i] = 1\n",
    "        else:\n",
    "            for j, p2 in enumerate(corpus.keys()):\n",
    "                if p2 in corpus[p1]:\n",
    "                    # ignore the link to its own\n",
    "                    binaryLinks[j, i] = 1\n",
    "    \n",
    "    # Start: 1 / N for every page\n",
    "    current = numpy.ones(N) / N\n",
    "    #print(current)\n",
    "    \n",
    "    # Repeatedly update rank values until no change in values > 0.001.\n",
    "    difference = 1\n",
    "    while difference > 0.001: \n",
    "        new = (1 - damping_factor) * numpy.ones(N) / N + \\\n",
    "          (damping_factor) * numpy.sum(binaryLinks * prob * current, axis = 1)\n",
    "        difference = max(abs(current - new))\n",
    "        current = new\n",
    "    \n",
    "    result = dict()\n",
    "    for i, page in enumerate(corpus):\n",
    "        result[page] = new[i]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bibliographic-teddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank Results from Sampling (n = 10000)\n",
      "  1.html: 0.2201\n",
      "  2.html: 0.4291\n",
      "  3.html: 0.2214\n",
      "  4.html: 0.1294\n",
      "PageRank Results from Iteration\n",
      "  1.html: 0.2198\n",
      "  2.html: 0.4294\n",
      "  3.html: 0.2198\n",
      "  4.html: 0.1311\n"
     ]
    }
   ],
   "source": [
    "%run pagerank.py \"corpus0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "palestinian-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank Results from Sampling (n = 10000)\n",
      "  bfs.html: 0.1165\n",
      "  dfs.html: 0.0774\n",
      "  games.html: 0.2287\n",
      "  minesweeper.html: 0.1162\n",
      "  minimax.html: 0.1308\n",
      "  search.html: 0.2126\n",
      "  tictactoe.html: 0.1178\n",
      "PageRank Results from Iteration\n",
      "  bfs.html: 0.1152\n",
      "  dfs.html: 0.0809\n",
      "  games.html: 0.2277\n",
      "  minesweeper.html: 0.1180\n",
      "  minimax.html: 0.1312\n",
      "  search.html: 0.2090\n",
      "  tictactoe.html: 0.1180\n"
     ]
    }
   ],
   "source": [
    "%run pagerank.py \"corpus1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "naked-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank Results from Sampling (n = 10000)\n",
      "  ai.html: 0.1931\n",
      "  algorithms.html: 0.1056\n",
      "  c.html: 0.1214\n",
      "  inference.html: 0.1333\n",
      "  logic.html: 0.0280\n",
      "  programming.html: 0.2258\n",
      "  python.html: 0.1193\n",
      "  recursion.html: 0.0735\n",
      "PageRank Results from Iteration\n",
      "  ai.html: 0.1889\n",
      "  algorithms.html: 0.1064\n",
      "  c.html: 0.1238\n",
      "  inference.html: 0.1289\n",
      "  logic.html: 0.0264\n",
      "  programming.html: 0.2301\n",
      "  python.html: 0.1238\n",
      "  recursion.html: 0.0717\n"
     ]
    }
   ],
   "source": [
    "%run pagerank.py \"corpus2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "coated-relationship",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.html': 0.4543820027434842,\n",
       " '2.html': 0.4543820027434842,\n",
       " '3.html': 0.09123599451303153}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_pagerank(toy, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "curious-symbol",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1.html': 0.4504, '2.html': 0.4558, '3.html': 0.0938})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = {'1.html': {'2.html'}, \n",
    "       '2.html': {'1.html', '2.html'}, \n",
    "       '3.html': set()}\n",
    "sample_pagerank(toy, 0.8, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "novel-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangj337\\Google Drive\\Courses\\IntroAI-CS50\\2Uncertainty\\pagerank\n",
      "{'1.html': {'2.html'}, '2.html': {'3.html', '1.html'}, '3.html': {'4.html', '2.html'}, '4.html': {'2.html'}}\n",
      "4\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd C:/Users/wangj337/Google Drive/Courses/IntroAI-CS50/2Uncertainty/pagerank/\n",
    "corpus = crawl(\"corpus0\")\n",
    "print(corpus)\n",
    "print(len(corpus))\n",
    "print(len(corpus['2.html']))\n",
    "test = transition_model(corpus, '1.html', 0.6)\n",
    "sum(test.values())\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dramatic-benjamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.html': set(), '2.html': {'3.html', '1.html'}, '3.html': {'4.html', '2.html'}, '4.html': {'4.html', '2.html'}}\n",
      "[[1. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4., 2., 2., 1.])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    corpus['1.html'] = set()\n",
    "    corpus['4.html'] = {'4.html', '2.html'}\n",
    "\n",
    "    N = len(corpus)\n",
    "    numLinks = numpy.array([len(corpus[p]) for p in corpus], dtype=float)\\\n",
    "      - numpy.array([(p in corpus[p]) for p in corpus], dtype=float)\n",
    "    numLinks[numLinks == 0] = N\n",
    "    prob = numpy.reciprocal(numLinks)\n",
    "    binaryLinks = numpy.zeros((N, N))\n",
    "    for i, p1 in enumerate(corpus.keys()):\n",
    "        if not corpus[p1]:\n",
    "            for j in range(N):\n",
    "                binaryLinks[j, i] = 1\n",
    "        else:\n",
    "            for j, p2 in enumerate(corpus.keys()):\n",
    "                if p2 in corpus[p1] and p1 != p2:\n",
    "                    # ignore the link to its own\n",
    "                    binaryLinks[j, i] = 1\n",
    "\n",
    "print(corpus)                    \n",
    "print(binaryLinks)\n",
    "#numpy.sum(binaryLinks, axis = 0)\n",
    "numLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "million-british",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 2. 1.]\n",
      "[1.  0.5 0.5 1. ]\n",
      "[100.   2.   2. 100.]\n",
      "[0. 1. 2. 3.]\n",
      "306.0\n",
      "[100.   1.   0.  97.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = numpy.array([len(corpus[p]) for p in corpus], dtype=float)\n",
    "print(a)\n",
    "print(numpy.reciprocal(a))\n",
    "b = numpy.array([0, 1, 2, 3], dtype=float)\n",
    "a[a == 1] = 100\n",
    "print(a)\n",
    "print(b)\n",
    "print(numpy.inner(a, b))\n",
    "print(abs(b - a))\n",
    "max(abs(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "hydraulic-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(corpus)\n",
    "current = numpy.ones(N) / N\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "classified-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.html': 0.1, '2.html': 0.7, '3.html': 0.1, '4.html': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-5dfdde2a5716>:2: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  keys = random.sample(test.keys(), 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.html'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test)\n",
    "keys = random.sample(test.keys(), 1)\n",
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eligible-density",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1.html', '2.html', '3.html', '4.html'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "recreational-occasion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.1, 0.7, 0.1, 0.1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "italian-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cherry\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'4.html': 0.15, '2.html': 0.71, '1.html': 0.08, '3.html': 0.06})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "print(random.choice(mylist))\n",
    "\n",
    "res = random.choices(\n",
    "    population=list(test.keys()),\n",
    "    weights=test.values(),\n",
    "    k=100\n",
    ")\n",
    "frequency = Counter(res)\n",
    "\n",
    "for p in frequency:\n",
    "    frequency[p] = frequency[p] / 100\n",
    "\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "round-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.html']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list()\n",
    "    \n",
    "current = random.choice(list(test.keys()))\n",
    "result.append(current)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ultimate-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.html'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "national-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook scratch.ipynb to script\n",
      "[NbConvertApp] Writing 8326 bytes to pagerank.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script \"scratch.ipynb\" --output pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-ferry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
